{
 "metadata": {
  "name": "",
  "signature": "sha256:f3b325d4dd5914536f8bbbdd1520fbf35a87e6556538bdbcaafb0ba21eb27e4c"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Homework #4"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Our final homework was to implement the string model using MPI. As with my CUDA version, I sacrificed all file input and output to concentrate on simply producing correct results.\n",
      "\n",
      "As with the CUDA code, my code relies on the number of processors evenly dividing the number of masses. I think it is not too hard to make more general code, but to learn the general idea of using MPI it is not necessary.\n",
      "\n",
      "If you go back to my CUDA code, one thing you will see is that the code is actually extremely simple and compact:\n",
      "\n",
      "```\n",
      "__global__ void make_move(float *y, float *yold, float *v, float *out, int time){\n",
      "\n",
      "  out[time] = yold[NUM_MASSES/2];\n",
      "\n",
      "  int i = blockIdx.x*blockDim.x + threadIdx.x;\n",
      "  float Ktension = 0.2;\n",
      "  float Kdamping = 0.9999;\n",
      "\n",
      "  if ( i > 0 && i < NUM_MASSES-1 ) {\n",
      "    float accel = Ktension * (yold[i+1] + yold[i-1] - 2*yold[i]);\n",
      "    v[i] += accel;\n",
      "    v[i] *= Kdamping;\n",
      "    y[i] = yold[i] + v[i];\n",
      "  }\n",
      "}\n",
      "```\n",
      "\n",
      "On the other hand, the MPI code is a complete mess. I had to split it into three parts: two parts to handle the first and last processes and another part to handle all the middle parts. All three parts had slightly different code, with, most annoyingly, slightly dfferent ways to index each processes' individual array, which made it extremely easy to introduce off by one index errors.\n",
      "\n",
      "However, I believe I was able to produce correct output :)\n",
      "\n",
      "I've attached my source code to the file. The way I am copiling is with the following:\n",
      "\n",
      "mpicc -o string_mpi string_mpi.c -std=c99\n",
      "\n",
      "and to run the code I use:\n",
      "\n",
      "mpirun -np 16 ./string_mpi\n",
      "\n",
      "What I wanted to see was output that was the same as the cuda output:\n",
      "\n",
      "<b>\n",
      "y[NUM_MASSES/2] 1.000000  1.000000\n",
      "\n",
      "y[NUM_MASSES/2] 0.600040  0.600040\n",
      "\n",
      "</b><b>\n",
      "\n",
      "y[NUM_MASSES/2] 0.040112  0.040112</b><b>\n",
      "\n",
      "y[NUM_MASSES/2] -0.359840  -0.359840</b><b>\n",
      "\n",
      "y[NUM_MASSES/2] -0.407869  -0.407869</b><b>\n",
      "\n",
      "y[NUM_MASSES/2] -0.152608  -0.152608</b><b>\n",
      "\n",
      "y[NUM_MASSES/2] 0.172032  0.172032</b><b>\n",
      "\n",
      "y[NUM_MASSES/2] 0.322525  0.322525</b><b>\n",
      "\n",
      "y[NUM_MASSES/2] 0.212364  0.212364</b><b>\n",
      "\n",
      "y[NUM_MASSES/2] -0.045944  -0.045944</b><b>\n",
      "\n",
      "y[NUM_MASSES/2] -0.243441  -0.243441</b><b>\n",
      "\n",
      "y[NUM_MASSES/2] -0.237040  -0.237040</b><b>\n",
      "\n",
      "y[NUM_MASSES/2] -0.049417  -0.049417</b><b>\n",
      "\n",
      "y[NUM_MASSES/2] 0.161809  0.161808</b><b>\n",
      "\n",
      "y[NUM_MASSES/2] 0.233159  0.233158</b><b>\n",
      "\n",
      "y[NUM_MASSES/2] 0.119496  0.119496</b><b>\n",
      "\n",
      "y[NUM_MASSES/2] -0.079609  -0.079609</b><b>\n",
      "\n",
      "y[NUM_MASSES/2] -0.205132  -0.205132</b><b>\n",
      "\n",
      "y[NUM_MASSES/2] -0.164094  -0.164095</b><b>\n",
      "\n",
      "y[NUM_MASSES/2] 0.002631  0.002631\n",
      "\n",
      "</b>\n",
      "\n",
      "However, obvious in retrospect, because the masses are evenly divided by the number of processes, the middle node is by definition going to be processed or at least used by two different processes, therefore my code prints these values twice and gives the following output:\n",
      "\n",
      "my_rank 7 1.000000\n",
      "\n",
      "my_rank 7 0.600040\n",
      "\n",
      "my_rank 7 0.040112\n",
      "\n",
      "my_rank 8 1.000000\n",
      "\n",
      "my_rank 8 0.600040\n",
      "\n",
      "my_rank 7 -0.359840\n",
      "\n",
      "my_rank 7 -0.407869\n",
      "\n",
      "my_rank 7 -0.152608\n",
      "\n",
      "my_rank 8 0.040112\n",
      "\n",
      "my_rank 8 -0.359840\n",
      "\n",
      "my_rank 8 -0.407869\n",
      "\n",
      "my_rank 8 -0.152608\n",
      "\n",
      "my_rank 8 0.172032\n",
      "\n",
      "my_rank 8 0.322525\n",
      "\n",
      "my_rank 8 0.212364\n",
      "\n",
      "my_rank 7 0.172032\n",
      "\n",
      "my_rank 7 0.322525\n",
      "\n",
      "my_rank 7 0.212364\n",
      "\n",
      "my_rank 8 -0.045943\n",
      "\n",
      "my_rank 8 -0.243441\n",
      "\n",
      "my_rank 8 -0.237040\n",
      "\n",
      "my_rank 8 -0.049417\n",
      "\n",
      "my_rank 8 0.161809\n",
      "\n",
      "my_rank 8 0.233159\n",
      "\n",
      "my_rank 8 0.119496\n",
      "\n",
      "my_rank 8 -0.079609\n",
      "\n",
      "my_rank 8 -0.205132\n",
      "\n",
      "my_rank 8 -0.164094\n",
      "\n",
      "my_rank 7 -0.045943\n",
      "\n",
      "my_rank 7 -0.243441\n",
      "\n",
      "my_rank 7 -0.237040\n",
      "\n",
      "my_rank 7 -0.049417\n",
      "\n",
      "my_rank 7 0.161809\n",
      "\n",
      "my_rank 7 0.233159\n",
      "\n",
      "my_rank 7 0.119496\n",
      "\n",
      "my_rank 7 -0.079609\n",
      "\n",
      "my_rank 7 -0.205132\n",
      "\n",
      "my_rank 7 -0.164094\n",
      "\n",
      "my_rank 7 0.002631\n",
      "\n",
      "my_rank 8 0.002631\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Which is the same numbers as CUDA, but repeated twice and sligtly disarranged."
     ]
    }
   ],
   "metadata": {}
  }
 ]
}